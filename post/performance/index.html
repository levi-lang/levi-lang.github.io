<!DOCTYPE html>
<html lang="en-us">
    <head>
        

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>性能之巅-CPU</title>
        
        <style>

    html body {
        font-family: 'Raleway', sans-serif;
        background-color: white;
    }

    :root {
        --accent: red;
        --border-width:  5px ;
    }

</style>


<link rel="stylesheet" href="https://levi-lang.github.io/css/main.css">





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css" crossorigin="anonymous">


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css" integrity="sha512-+4zCK9k+qNFUR5X+cKL9EIR+ZOhtIloNl9GIKS57V1MyNsYpYcUrUeQc9vNfzsWfV28IaLL3i96P9sdNyeRssA==" crossorigin="anonymous" />
 

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/go.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/kotlin.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/swift.min.js"></script>
    
    <script>hljs.initHighlightingOnLoad();</script>






<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>
 <meta name="generator" content="Hugo 0.101.0" />
        

        

        
            <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        

        

    </head>

    <body>
        

        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container">
                <div class="navbar-header">
                    <a class="navbar-brand visible-xs" href="#">性能之巅-CPU</a>
                    <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                </div>
                <div class="collapse navbar-collapse">
                    
                        <ul class="nav navbar-nav">
                            
                                <li><a href="/">Home</a></li>
                            
                                <li><a href="/about/">About</a></li>
                            
                                <li><a href="/post/">Posts</a></li>
                            
                                <li><a href="/project/">Projects</a></li>
                            
                        </ul>
                    
                    
                        <ul class="nav navbar-nav navbar-right">
                            
                                <li class="navbar-icon"><a href="mailto:me@example.com"><i class="fas fa-envelope"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://github.com/username/"><i class="fab fa-github"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://twitter.com/username/"><i class="fab fa-twitter"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://www.linkedin.com/in/username/"><i class="fab fa-linkedin"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://www.stackoverflow.com/username/"><i class="fab fa-stack-overflow"></i></a></li>
                            
                        </ul>
                    
                </div>
            </div>
        </nav>


<main>

    <div>
        <h2>性能之巅-CPU</h2>
        <h5>July 17, 2022</h5>
        

    </div>

    <div align="start" class="content"><h1 id="问题">问题</h1>
<pre tabindex="0"><code>1、CPU 密集型进程 导致平均负载升高
2、I/O 密集型进程 导致平均负载升高
3、大量等待 CPU 的进程调度也会导致平均负载升高
4、进程或线程上下文切换过高也会导致凭据负载升高
5、代码使得进行导致cpu使用率过高
6、短进程导致cpu使用率过高
7、僵尸进程处理
8、软中断

cpu负载多少算高
cpu上下文切换多少算高
</code></pre><h1 id="概念">概念</h1>
<h2 id="平均负载与使用率">平均负载与使用率</h2>
<pre tabindex="0"><code>平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数
它和 CPU 使用率并没有直接关系。
这里我先解释下，
可运行状态和不可中断状态这俩词儿。

所谓可运行状态的进程，是指正在使用 CPU 或者正在等待 CPU 的进程
也就是我们常用 ps 命令看到的，处于 R 状态（Running 或 Runnable）的进程。

不可中断状态的进程则是正处于内核态关键流程中的进程，并且这些流程是不可打断的
比如最常见的是等待硬件设备的 I/O 响应
也就是我们在 ps 命令中看到的 D 状态（Uninterruptible Sleep，也称为 Disk Sleep）的进程。
比如，当一个进程向磁盘读写数据时，为了保证数据的一致性，在得到磁盘回复前
它是不能被其他进程或者中断打断的，这个时候的进程就处于不可中断状态。如果此时的进程被打断了
就容易出现磁盘数据与进程数据不一致的问题。

所以，不可中断状态实际上是系统对进程和硬件设备的一种保护机制。

因此，你可以简单理解为，平均负载其实就是平均活跃进程数。
平均活跃进程数，直观上的理解就是单位时间内的活跃进程数，
但它实际上是活跃进程数的指数衰减平均值。
这个“指数衰减平均”的详细含义你不用计较，
这只是系统的一种更快速的计算方式，你把它直接当成活跃进程数的平均值也没问题。

既然平均的是活跃进程数，那么最理想的，就是每个 CPU 上都刚好运行着一个进程，
这样每个 CPU 都得到了充分利用。
比如当平均负载为 2 时，意味着什么呢？
  在只有 2 个 CPU 的系统上，意味着所有的 CPU 都刚好被完全占用。
  在 4 个 CPU 的系统上，意味着 CPU 有 50% 的空闲。
  而在只有 1 个 CPU 的系统中，则意味着有一半的进程竞争不到 CPU。

平均负载为多少时合理
我们知道，平均负载最理想的情况是等于 CPU 个数。
所以在评判平均负载时，首先你要知道系统有几个 CPU，
这可以通过 top 命令或者从文件 /proc/cpuinfo 中读取
  grep &#39;model name&#39; /proc/cpuinfo | wc -l

我们在例子中可以看到，平均负载有三个数值，到底该参考哪一个呢
  如果 1 分钟、5 分钟、15 分钟的三个值基本相同，或者相差不大，那就说明系统负载很平稳。
  但如果 1 分钟的值远小于 15 分钟的值，就说明系统最近 1 分钟的负载在减少，而过去 15 分钟内却有很大的负载。
  反过来，如果 1 分钟的值远大于 15 分钟的值，就说明最近 1 分钟的负载在增加
  这种增加有可能只是临时性的，也有可能还会持续增加下去，所以就需要持续观察。
  一旦 1 分钟的平均负载接近或超过了 CPU 的个数，就意味着系统正在发生过载的问题
  这时就得分析调查是哪里导致的问题，并要想办法优化了

在我看来，当平均负载高于 CPU 数量 70% 的时候，你就应该分析排查负载高的问题了

平均负载与 CPU 使用率
我们还是要回到平均负载的含义上来，平均负载是指单位时间内，处于可运行状态和不可中断状态的进程数。
所以，它不仅包括了正在使用 CPU 的进程，还包括等待 CPU 和等待 I/O 的进程。
而 CPU 使用率，是单位时间内 CPU 繁忙情况的统计，跟平均负载并不一定完全对应。
比如：

  CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的；
  I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高；
  大量等待 CPU 的进程调度也会导致平均负载升高，此时的 CPU 使用率也会比较高。

平均负载提供了一个快速查看系统整体性能的手段，反映了整体的负载情况。
但只看平均负载本身，我们并不能直接发现，到底是哪里出现了瓶颈。
所以，在理解平均负载时，也要注意：
  平均负载高有可能是 CPU 密集型进程导致的；
  平均负载高并不一定代表 CPU 使用率高，还有可能是 I/O 更繁忙了
  当发现负载高的时候，你可以使用 mpstat、pidstat 等工具，辅助分析负载的来源

iowait 高不一定代表 I/O 有性能瓶颈。
当系统中只有 I/O 类型的进程在运行时，iowait 也会很高，但实际上，磁盘的读写远没有达到性能瓶颈的程度。
因此，碰到 iowait 升高时，需要先用 dstat、pidstat 等工具，确认是不是磁盘 I/O 的问题，然后再找是哪些进程导致了 I/O。
等待 I/O 的进程一般是不可中断状态，所以用 ps 命令找到的 D 状态（即不可中断状态）的进程，多为可疑进程
</code></pre><h2 id="上下文切换">上下文切换</h2>
<pre tabindex="0"><code>多个进程竞争 CPU 就是一个经常被我们忽视的问题
进程在竞争 CPU 的时候并没有真正运行，为什么还会导致系统的负载升高呢？
CPU 上下文切换就是罪魁祸首

Linux 是一个多任务操作系统，它支持远大于 CPU 数量的任务同时运行。
当然，这些任务实际上并不是真的在同时运行，而是因为系统在很短的时间内，将 CPU 轮流分配给它们，造成多任务同时运行的错觉

# cpu上下文
而在每个任务运行前，CPU 都需要知道任务从哪里加载、又从哪里开始运行
也就是说，需要系统事先帮它设置好 CPU 寄存器和程序计数器（Program Counter，PC）。
CPU 寄存器，是 CPU 内置的容量小、但速度极快的内存。
而程序计数器，则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。
它们都是 CPU 在运行任何任务前，必须的依赖环境，因此也被叫做 CPU 上下文。

所以，根据任务的不同，CPU 的上下文切换就可以分为几个不同的场景
  进程上下文切换
  线程上下文切换
  中断上下文切换

进程既可以在用户空间运行，又可以在内核空间中运行。
进程在用户空间运行时，被称为进程的用户态，而陷入内核空间的时候，被称为进程的内核态。

从用户态到内核态的转变，需要通过系统调用来完成。
比如，当我们查看文件内容时，就需要多次系统调用来完成：
  首先调用 open() 打开文件，然后调用 read() 读取文件内容，并调用 write() 将内容写到标准输出，最后再调用 close() 关闭文件。
  而系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程。

那么，系统调用的过程有没有发生 CPU 上下文的切换呢？答案自然是肯定的。
  CPU 寄存器里原来用户态的指令位置，需要先保存起来。接着，为了执行内核态代码，
  CPU 寄存器需要更新为内核态指令的新位置。最后才是跳转到内核态运行内核任务。

所以，一次系统调用的过程，其实是发生了两次 CPU 上下文切换。
不过，需要注意的是
系统调用过程中，并不会涉及到虚拟内存等进程用户态的资源，也不会切换进程。
这跟我们通常所说的进程上下文切换是不一样的

  进程上下文切换，是指从一个进程切换到另一个进程运行。
  而系统调用过程中一直是同一个进程在运行。

所以，系统调用过程通常称为特权模式切换，而不是上下文切换。
但实际上，系统调用过程中，CPU 的上下文切换还是无法避免的。

其实还有很多其他场景，也会触发进程调度，在这里我给你逐个梳理下。
  其一，为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。
  这样，当某个进程的时间片耗尽了，就会被系统挂起，切换到其它正在等待 CPU 的进程运行。

  其二，进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，
  这个时候进程也会被挂起，并由系统调度其他进程运行。

  其三，当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度。
  其四，当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行。
  
  最后一个，发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。

对同一个 CPU 来说，中断处理比进程拥有更高的优先级

总结一下，不管是哪种场景导致的上下文切换，你都应该知道：
CPU 上下文切换，是保证 Linux 系统正常工作的核心功能之一，
一般情况下不需要我们特别关注。
但过多的上下文切换，会把 CPU 时间消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，
从而缩短进程真正运行的时间，导致系统的整体性能大幅下降。

所谓自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换。
比如说， I/O、内存等系统资源不足时，就会发生自愿上下文切换。

而非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。
比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换。

上下文切换频率是多少次才算正常呢
这个数值其实取决于系统本身的 CPU 性能。
在我看来，如果系统的上下文切换次数比较稳定，那么从数百到一万以内，都应该算是正常的。
但当上下文切换次数超过一万次，或者切换次数出现数量级的增长时，就很可能已经出现了性能问题。
这时，你还需要根据上下文切换的类型，再做具体分析。
比方说：
  自愿上下文切换变多了，说明进程都在等待资源，有可能发生了 I/O 等其他问题；
  非自愿上下文切换变多了，说明进程都在被强制调度，也就是都在争抢 CPU，说明 CPU 的确成了瓶颈；
  中断次数变多了，说明 CPU 被中断处理程序占用，还需要通过查看 /proc/interrupts 文件来分析具体的中断类型。


既然是中断，我们都知道，它只发生在内核态
而 pidstat 只是一个进程的性能分析工具，并不提供任何关于中断的详细信息，怎样才能知道中断发生的类型呢？
没错，那就是从 /proc/interrupts 这个只读文件中读取。
/proc 实际上是 Linux 的一个虚拟文件系统，用于内核空间与用户空间之间的通信。
/proc/interrupts 就是这种通信机制的一部分，提供了一个只读的中断使用情况。
</code></pre><h2 id="使用率">使用率</h2>
<pre tabindex="0"><code>CPU 使用率，就是除了空闲时间外的其他时间占总 CPU 时间的百分比
  cpu使用率=1 - 空闲时间/总cpu时间

根据这个公式，我们就可以从 /proc/stat 中的数据，很容易地计算出 CPU 使用率

不过先不要着急计算，你能说出，直接用 /proc/stat 的数据，算的是什么时间段的 CPU 使用率吗？
看到这里，你应该想起来了，这是开机以来的节拍数累加值，所以直接算出来的，是开机以来的平均 CPU 使用率，一般没啥参考价值。
事实上，为了计算 CPU 使用率，性能工具一般都会取间隔一段时间（比如 3 秒）的两次值，作差后，再计算出这段时间内的平均 CPU 使用率，
即
  平均cpu使用率=1 - 空闲时间（new）-空闲时间（old）/总cpu时间（new）-总cpu时间（old）
这个公式，就是我们用各种性能工具所看到的 CPU 使用率的实际计算方法

不过要注意的是，性能分析工具给出的都是间隔一段时间的平均 CPU 使用率
所以要注意间隔时间的设置，特别是用多个工具对比分析时，你一定要保证它们用的是相同的间隔时间
比如，对比一下 top 和 ps 这两个工具报告的 CPU 使用率，
默认的结果很可能不一样，因为 top 默认使用 3 秒时间间隔，而 ps 使用的却是进程的整个生命周期。

那么哪种工具适合在第一时间分析进程的 CPU 问题呢？
我的推荐是 perf。
perf 是 Linux 2.6.31 以后内置的性能分析工具。
它以性能事件采样为基础，不仅可以分析系统的各种事件和内核性能，还可以用来分析指定应用程序的性能问题。

CPU 使用率是最直观和最常用的系统性能指标，更是我们在排查性能问题时，通常会关注的第一个指标。
所以我们更要熟悉它的含义，
尤其要弄清楚用户（%user）、Nice（%nice）、系统（%system） 、等待 I/O（%iowait） 
中断（%irq）以及软中断（%softirq）这几种不同 CPU 的使用率。
比如说：
  用户 CPU 和 Nice CPU 高，说明用户态进程占用了较多的 CPU，所以应该着重排查进程的性能问题。
  系统 CPU 高，说明内核态占用了较多的 CPU，所以应该着重排查内核线程或者系统调用的性能问题。
  I/O 等待 CPU 高，说明等待 I/O 的时间比较长，所以应该着重排查系统存储是不是出现了 I/O 问题。
  软中断和硬中断高，说明软中断或硬中断的处理程序占用了较多的 CPU，所以应该着重排查内核中的中断服务程序。
  碰到 CPU 使用率升高的问题，你可以借助 top、pidstat 等工具
  确认引发 CPU 性能问题的来源；再使用 perf 等工具，排查出引起性能问题的具体函数

碰到常规问题无法解释的 CPU 使用率情况时，首先要想到有可能是短时应用导致的问题
比如有可能是下面这两种情况。
  第一，应用里直接调用了其他二进制程序，这些程序通常运行时间比较短，通过 top 等工具也不容易发现。
  第二，应用本身在不停地崩溃重启，而启动过程的资源初始化，很可能会占用相当多的 CPU。
  对于这类进程，我们可以用 pstree 或者 execsnoop 找到它们的父进程，再从父进程所在的应用入手，排查问题的根源。
</code></pre><h2 id="进程状态">进程状态</h2>
<pre tabindex="0"><code>当 iowait 升高时，进程很可能因为得不到硬件的响应，而长时间处于不可中断状态。
从 ps 或者 top 命令的输出中，你可以发现它们都处于 D 状态，也就是不可中断状态（Uninterruptible Sleep）。
既然说到了进程的状态，进程有哪些状态你还记得吗？

top 和 ps 是最常用的查看进程状态的工具，我们就从 top 的输出开始。
下面是一个 top 命令输出的示例，S 列（也就是 Status 列）表示进程的状态。
从这个示例里，你可以看到 R、D、Z、S、I 等几个状态，它们分别是什么意思呢？


$ top
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
28961 root      20   0   43816   3148   4040 R   3.2  0.0   0:00.01 top
  620 root      20   0   37280  33676    908 D   0.3  0.4   0:00.01 app
    1 root      20   0  160072   9416   6752 S   0.0  0.1   0:37.64 systemd
 1896 root      20   0       0      0      0 Z   0.0  0.0   0:00.00 devapp
    2 root      20   0       0      0      0 S   0.0  0.0   0:00.10 kthreadd
    4 root       0 -20       0      0      0 I   0.0  0.0   0:00.00 kworker/0:0H
    6 root       0 -20       0      0      0 I   0.0  0.0   0:00.00 mm_percpu_wq
    7 root      20   0       0      0      0 S   0.0  0.0   0:06.37 ksoftirqd/0

R 是 Running 或 Runnable 的缩写，表示进程在 CPU 的就绪队列中，正在运行或者正在等待运行。
D 是 Disk Sleep 的缩写，也就是不可中断状态睡眠（Uninterruptible Sleep），
一般表示进程正在跟硬件交互，并且交互过程不允许被其他进程或中断打断。

Z 是 Zombie 的缩写，如果你玩过“植物大战僵尸”这款游戏，应该知道它的意思。
它表示僵尸进程，也就是进程实际上已经结束了，但是父进程还没有回收它的资源（比如进程的描述符、PID 等）。

S 是 Interruptible Sleep 的缩写，也就是可中断状态睡眠，表示进程因为等待某个事件而被系统挂起。
当进程等待的事件发生时，它会被唤醒并进入 R 状态。

I 是 Idle 的缩写，也就是空闲状态，用在不可中断睡眠的内核线程上。
前面说了，硬件交互导致的不可中断进程用 D 表示，但对某些内核线程来说，它们有可能实际上并没有任何负载，用 Idle 正是为了区分这种情况。

要注意，D 状态的进程会导致平均负载升高， I 状态的进程却不会。
当然了，上面的示例并没有包括进程的所有状态。
除了以上 5 个状态，进程还包括下面这 2 个状态。

第一个是 T 或者 t，也就是 Stopped 或 Traced 的缩写，表示进程处于暂停或者跟踪状态。

向一个进程发送 SIGSTOP 信号，它就会因响应这个信号变成暂停状态（Stopped）；
再向它发送 SIGCONT 信号，进程又会恢复运行（如果进程是终端里直接启动的，则需要你用 fg 命令，恢复到前台运行）。

而当你用调试器（如 gdb）调试一个进程时，在使用断点中断进程后，进程就会变成跟踪状态，
这其实也是一种特殊的暂停状态，只不过你可以用调试器来跟踪并按需要控制进程的运行。

另一个是 X，也就是 Dead 的缩写，表示进程已经消亡，所以你不会在 top 或者 ps 命令中看到它

先看不可中断状态，这其实是为了保证进程数据与硬件状态一致，并且正常情况下，不可中断状态在很短时间内就会结束。
所以，短时的不可中断状态进程，我们一般可以忽略



$ ps aux | grep /app
root      4009  0.0  0.0   4376  1008 pts/0    Ss+  05:51   0:00 /app
root      4287  0.6  0.4  37280 33660 pts/0    D+   05:54   0:00 /app
root      4288  0.6  0.4  37280 33668 pts/0    D+   05:54   0:00 /app
从这个界面，我们可以发现多个 app 进程已经启动，并且它们的状态分别是 Ss+ 和 D+。
其中，S 表示可中断睡眠状态，D 表示不可中断睡眠状态，
我们在前面刚学过，那后面的 s 和 + 是什么意思呢？
不知道也没关系，查一下 man ps 就可以。
现在记住，s 表示这个进程是一个会话的领导进程，而 + 表示前台进程组。

这里又出现了两个新概念，进程组和会话。它们用来管理一组相互关联的进程，意思其实很好理解。
进程组表示一组相互关联的进程，比如每个子进程都是父进程所在组的成员；
而会话是指共享同一个控制终端的一个或多个进程组。

比如，我们通过 SSH 登录服务器，就会打开一个控制终端（TTY），这个控制终端就对应一个会话。
而我们在终端中运行的命令以及它们的子进程，就构成了一个个的进程组，
其中，在后台运行的命令，构成后台进程组；在前台运行的命令，构成前台进程组。

总结
不可中断状态，表示进程正在跟硬件交互，为了保护进程数据和硬件的一致性，系统不允许其他进程或中断打断这个进程。
进程长时间处于不可中断状态，通常表示系统有 I/O 性能问题。

僵尸进程表示进程已经退出，但它的父进程还没有回收子进程占用的资源。
短暂的僵尸状态我们通常不必理会，但进程长时间处于僵尸状态，就应该注意了，可能有应用程序没有正常处理子进程的退出
</code></pre><h2 id="中断">中断</h2>
<pre tabindex="0"><code>中断是系统用来响应硬件设备请求的一种机制，它会打断进程的正常调度和执行，然后调用内核中的中断处理程序来响应设备的请求
其实除了 iowait，软中断（softirq）CPU 使用率升高也是最常见的一种性能问题
中断其实是一种异步的事件处理机制，可以提高系统的并发处理能力。
由于中断处理程序会打断其他进程的运行，所以，为了减少对正常进程运行调度的影响，中断处理程序就需要尽可能快地运行。
如果中断本身要做的事情不多，那么处理起来也不会有太大问题；但如果中断要处理的事情很多，中断服务程序就有可能要运行很长时间。
特别是，中断处理程序在响应中断时，还会临时关闭中断。这就会导致上一次中断处理完成之前，其他中断都不能响应，也就是说中断有可能会丢失。

Linux 将中断处理过程分成了两个阶段，也就是上半部和下半部：
上半部用来快速处理中断，它在中断禁止模式下运行，主要处理跟硬件紧密相关的或时间敏感的工作。
下半部用来延迟处理上半部未完成的工作，通常以内核线程的方式运行。

网卡接收到数据包后，会通过硬件中断的方式，通知内核有新的数据到了。这时，内核就应该调用中断处理程序来响应它。
你可以自己先想一下，这种情况下的上半部和下半部分别负责什么工作呢？

对上半部来说，既然是快速处理，其实就是要把网卡的数据读到内存中，然后更新一下硬件寄存器的状态（表示数据已经读好了），
最后再发送一个软中断信号，通知下半部做进一步的处理。

而下半部被软中断信号唤醒后，需要从内存中找到网络数据，再按照网络协议栈，对数据进行逐层解析和处理，直到把它送给应用程序。

所以，这两个阶段你也可以这样理解：
上半部直接处理硬件请求，也就是我们常说的硬中断，特点是快速执行；
而下半部则是由内核触发，也就是我们常说的软中断，特点是延迟执行。

Linux 中的软中断包括网络收发、定时、调度、RCU 锁等各种类型，可以通过查看 /proc/softirqs 来观察软中断的运行情况。

$ cat /proc/softirqs
                    CPU0       CPU1
          HI:          0          0
       TIMER:     811613    1972736
      NET_TX:         49          7
      NET_RX:    1136736    1506885
       BLOCK:          0          0
    IRQ_POLL:          0          0
     TASKLET:     304787       3691
       SCHED:     689718    1897539
     HRTIMER:          0          0
         RCU:    1330771    1354737
在查看 /proc/softirqs 文件内容时，你要特别注意以下这两点。

第一，要注意软中断的类型，也就是这个界面中第一列的内容。
从第一列你可以看到，软中断包括了 10 个类别，分别对应不同的工作类型。
比如 NET_RX 表示网络接收中断，而 NET_TX 表示网络发送中断。

第二，要注意同一种软中断在不同 CPU 上的分布情况，也就是同一行的内容。
正常情况下，同一种中断在不同 CPU 上的累积次数应该差不多。
比如这个界面中，NET_RX 在 CPU0 和 CPU1 上的中断次数基本是同一个数量级，相差不大

在 Linux 中，每个 CPU 都对应一个软中断内核线程，名字是 ksoftirqd/CPU 编号。
当软中断事件的频率过高时，内核线程也会因为 CPU 使用率过高而导致软中断处理不及时，进而引发网络收发延迟、调度缓慢等性能问题。
</code></pre><h2 id="指标">指标</h2>
<pre tabindex="0"><code>使用率
  用户 CPU 使用率，包括用户态 CPU 使用率（user）和低优先级用户态 CPU 使用率（nice），表示 CPU 在用户态运行的时间百分比。用户 CPU 使用率高，通常说明有应用程序比较繁忙。
  系统 CPU 使用率，表示 CPU 在内核态运行的时间百分比（不包括中断）。系统 CPU 使用率高，说明内核比较繁忙。
  等待 I/O 的 CPU 使用率，通常也称为 iowait，表示等待 I/O 的时间百分比。iowait 高，通常说明系统与硬件设备的 I/O 交互时间比较长。
  软中断和硬中断的 CPU 使用率，分别表示内核调用软中断处理程序、硬中断处理程序的时间百分比。它们的使用率高，通常说明系统发生了大量的中断。
  除了上面这些，还有在虚拟化环境中会用到的窃取 CPU 使用率（steal）和客户 CPU 使用率（guest），分别表示被其他虚拟机占用的 CPU 时间百分比，和运行客户虚拟机的 CPU 时间百分比。
饱和度（平均负载）

进程上下文切换

CPU 缓存的命中率
</code></pre><h2 id="性能优化">性能优化</h2>
<pre tabindex="0"><code>第一，既然要做性能优化，那要怎么判断它是不是有效呢？特别是优化后，到底能提升多少性能呢？
第二，性能问题通常不是独立的，如果有多个性能问题同时发生，你应该先优化哪一个呢？
第三，提升性能的方法并不是唯一的，当有多种方法可以选择时，你会选用哪一种呢？
是不是总选那个最大程度提升性能的方法就行了呢？
在性能测试的领域，流传很广的一个说法是“二八原则”，也就是说 80% 的问题都是由 20% 的代码导致的。
只要找出这 20% 的位置，你就可以优化 80% 的性能。
所以，我想表达的是，并不是所有的性能问题都值得优化。

DPDK（Data Plane Development Kit）。
DPDK 是一种优化网络处理速度的方法，它通过绕开内核网络协议栈的方法，提升网络的处理能力。
不过它有一个很典型的要求，就是要独占一个 CPU 以及一定数量的内存大页，并且总是以 100% 的 CPU 使用率运行。
所以，如果你的 CPU 核数很少，就有点得不偿失了。

CPU 优化
  应用程序优化
  首先，从应用程序的角度来说，降低 CPU 使用率的最好方法当然是，排除所有不必要的工作，只保留最核心的逻辑。
  比如减少循环的层次、减少递归、减少动态内存分配等等。

  编译器优化
  算法优化
  异步处理
  多线程代替多进程
  善用缓存

  系统优化
  cpu绑定：把进程绑定到一个或者多个 CPU 上，可以提高 CPU 缓存的命中率，减少跨 CPU 调度带来的上下文切换问题。
  cpu独占：跟 CPU 绑定类似，进一步将 CPU 分组，并通过 CPU 亲和性机制为其分配进程。这样，这些 CPU 就由指定的进程独占，换句话说，不允许其他进程再来使用这些 CPU。
  优先级调整
  为进程设置资源限制
  numa 优化：支持 NUMA 的处理器会被划分为多个 node，每个 node 都有自己的本地内存空间。NUMA 优化，其实就是让 CPU 尽可能只访问本地内存。
  中断负载均衡：无论是软中断还是硬中断，它们的中断处理程序都可能会耗费大量的 CPU。开启 irqbalance 服务或者配置 smp_affinity，就可以把中断处理过程自动负载均衡到多个 CPU 上。
</code></pre><h1 id="cpu">CPU</h1>
<h2 id="watch-监测一个命令的运行结果">watch 监测一个命令的运行结果</h2>
<pre tabindex="0"><code>watch 可以帮助监测一个命令的运行结果，省得我们一遍遍地手动运行

查看当前目录文件 log 的变化
watch -d &#34;ls -l | grep log&#34;

每10秒查看一次系统的平均负载
watch -n10 cat /proc/loadavg
</code></pre><h2 id="uptime-平均负载">uptime 平均负载✅</h2>
<pre tabindex="0"><code>$ uptime
15:22:41 up 77 days,  3:42,  4 users,  load average: 1.02, 2.73, 3.22
//当前时间
//系统运行时间
//正在登录用户数
//而最后三个数字呢，依次则是过去 1 分钟、5 分钟、15 分钟的平均负载（Load Average）

$ watch -d uptime
# -d 参数表示高亮显示变化的区域
</code></pre><h2 id="tophtop-监测每个进程线程的cpu用量">top/htop 监测每个进程、线程的cpu用量✅</h2>
<pre tabindex="0"><code>略 top、iftop、iptop、htop、atop
</code></pre><h2 id="mpstat-单个cpu信息统计">mpstat 单个cpu信息统计✅</h2>
<pre tabindex="0"><code>mpstat 是一个常用的多核 CPU 性能分析工具，用来实时查看每个 CPU 的性能指标，以及所有 CPU 的平均指标

# -P ALL 表示监控所有CPU，后面数字5表示间隔5秒后输出一组数据
mpstat -P ALL 5
</code></pre><h2 id="pidstat-每个进程线程的cpu用量分解">pidstat 每个进程、线程的cpu用量分解✅</h2>
<pre tabindex="0"><code>pidstat 是一个常用的进程性能分析工具，用来实时查看进程的 CPU、内存、I/O 以及上下文切换等性能指标

# 间隔5秒后输出一组数据
$ pidstat -u 5 1
</code></pre><h2 id="vmstat-包括系统级的cpu平均负载">vmstat 包括系统级的CPU平均负载✅</h2>
<pre tabindex="0"><code>vmstat 是一个常用的系统性能分析工具，主要用来分析系统的内存使用情况
也常用来分析 CPU 上下文切换和中断的次数


# 每隔5秒输出1组数据
$ vmstat 5
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  0      0 7005360  91564 818900    0    0     0     0   25   33  0  0 100  0  0

cs（context switch）是每秒上下文切换的次数。
in（interrupt）则是每秒中断的次数。
r（Running or Runnable）是就绪队列的长度，也就是正在运行和等待 CPU 的进程数。
b（Blocked）则是处于不可中断睡眠状态的进程数

vmstat 只给出了系统总体的上下文切换情况，要想查看每个进程的详细情况，就需要使用我们前面提到过的 pidstat 了。
给它加上 -w 选项，你就可以查看每个进程上下文切换的情况了。


# 每隔5秒输出1组数据
$ pidstat -w 5
Linux 4.15.0 (ubuntu)  09/23/18  _x86_64_  (2 CPU)

08:18:26      UID       PID   cswch/s nvcswch/s  Command
08:18:31        0         1      0.20      0.00  systemd
08:18:31        0         8      5.40      0.00  rcu_sched

这个结果中有两列内容是我们的重点关注对象。
一个是 cswch ，表示每秒自愿上下文切换（voluntary context switches）的次数，
另一个则是 nvcswch ，表示每秒非自愿上下文切换（non voluntary context switches）的次数。
这两个概念你一定要牢牢记住，因为它们意味着不同的性能问题

# 每隔1秒输出1组数据（需要 Ctrl+C 才结束）
# -w参数表示输出进程切换指标，而-u参数则表示输出CPU使用指标$ pidstat -w -u 1
</code></pre><h2 id="perf-cpu剖析和pmc分析">perf cpu剖析和PMC分析✅</h2>
<pre tabindex="0"><code>测量IPC 作为运行周期低效的一个指标
使用 Perf 可以计算每个时钟周期内的指令数，称为 IPC，
IPC 偏低表明代码没有很好地利用 CPU。
Perf 还可以对程序进行函数级别的采样，从而了解程序的性能瓶颈究竟在哪里等等。
Perf 还可以替代 strace，可以添加动态内核 probe 点，还可以做 benchmark 衡量调度器的好坏

perf是内置于Linux内核源码树中的系统性能剖析(profiling)工具
原理是在CPU的PMU register中Get/Set performance counters来获得诸如instructions executed
cache-missed suffered，branches mispredicted等信息。

Perf是一个包含22种子工具的工具集，以下介绍最常用的5种：
perf list
perf stat
perf top
perf record
perf report

注意：性能事件的采集和分析请在物理机上进行，虚拟机很多性能事件屏蔽掉了。


# -g开启调用关系分析，-p指定php-fpm的进程号21515
$ perf top -g -p 21515
</code></pre><h2 id="ps-进程状态-">ps 进程状态 ✅</h2>
<pre tabindex="0"><code></code></pre><h2 id="pstree">pstree✅</h2>
<pre tabindex="0"><code>要怎么查找一个进程的父进程呢？
没错，用 pstree 就可以用树状形式显示所有进程之间的关系
</code></pre><h2 id="dstat">dstat✅</h2>
<pre tabindex="0"><code>可以同时查看 CPU 和 I/O 这两种资源的使用情况
</code></pre><h2 id="strace-">strace ✅</h2>
<pre tabindex="0"><code>strace 正是最常用的跟踪进程系统调用的工具
</code></pre><h2 id="sar-历史统计信息-">sar 历史统计信息 ✅</h2>
<pre tabindex="0"><code>sar 是一个系统活动报告工具，既可以实时查看系统的当前活动，又可以配置保存和报告历史统计数据

# -n DEV 表示显示网络收发的报告，间隔1秒输出一组数据
$ sar -n DEV 1
15:03:46        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
15:03:47         eth0  12607.00   6304.00    664.86    358.11      0.00      0.00      0.00      0.01
15:03:47      docker0   6302.00  12604.00    270.79    664.66      0.00      0.00      0.00      0.00
15:03:47           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
15:03:47    veth9f6bbcd   6302.00  12604.00    356.95    664.66      0.00      0.00      0.00      0.05

对于 sar 的输出界面，我先来简单介绍一下，从左往右依次是：
  第一列：表示报告的时间。
  第二列：IFACE 表示网卡。
  第三、四列：rxpck/s 和 txpck/s 分别表示每秒接收、发送的网络帧数，也就是 PPS。
  第五、六列：rxkB/s 和 txkB/s 分别表示每秒接收、发送的千字节数，也就是 BPS。
  后面的其他参数基本接近 0，显然跟今天的问题没有直接关系，你可以先忽略掉。
</code></pre><h2 id="hping3-">hping3 ✅</h2>
<pre tabindex="0"><code>hping3 是一个可以构造 TCP/IP 协议数据包的工具，可以对系统进行安全审计、防火墙测试等

运行 hping3 命令，来模拟 Nginx 的客户端请求
这是一个 SYN FLOOD 攻击

# -S参数表示设置TCP协议的SYN（同步序列号），-p表示目的端口为80
# -i u100表示每隔100微秒发送一个网络帧
# 注：如果你在实践过程中现象不明显，可以尝试把100调小，比如调成10甚至1
$ hping3 -S -p 80 -i u100 192.168.0.30
</code></pre><h2 id="tcpdump-">tcpdump ✅</h2>
<pre tabindex="0"><code>tcpdump 是一个常用的网络抓包工具，常用来分析各种网络问题

# -i eth0 只抓取eth0网卡，-n不解析协议名和主机名
# tcp port 80表示只抓取tcp协议并且端口号为80的网络帧
$ tcpdump -i eth0 -n tcp port 80
15:11:32.678966 IP 192.168.0.2.18238 &gt; 192.168.0.30.80: Flags [S], seq 458303614, win 512, length 0
</code></pre><h2 id="dmesg">dmesg</h2>
<pre tabindex="0"><code>检查cpu温度停滞信息 （cpu时钟频率限速）
</code></pre><h2 id="profile-cpu栈踪迹采集">profile cpu栈踪迹采集</h2>
<h2 id="showboost">showboost</h2>
<h2 id="turboboost-显示cpu时钟频率和其他状态">turboboost 显示cpu时钟频率和其他状态</h2>
<pre tabindex="0"><code>检查当前cpu 的时钟频率 以防太低
</code></pre><h2 id="iostat">iostat</h2>
<h2 id="pmcarch-显示告警cpu周期用量">pmcarch 显示告警cpu周期用量</h2>
<h2 id="lscpu">lscpu</h2>
<h2 id="time-给一个命令计时-含cpu用量分解">time 给一个命令计时 含cpu用量分解</h2>
<h2 id="ptime">ptime</h2>
<h2 id="tlbstat-总结tlb周期">tlbstat 总结TLB周期</h2>
<h2 id="cpudist-总结在cpu上运行的时间">cpudist 总结在cpu上运行的时间</h2>
<h2 id="runqlat-总结cpu运行队列延迟">runqlat 总结cpu运行队列延迟</h2>
<h2 id="runqlen-总结cpu运行队列长度">runqlen 总结cpu运行队列长度</h2>
<h2 id="softirqs-总结软中断时间">softirqs 总结软中断时间</h2>
<h2 id="hardirqs-总结硬中断时间">hardirqs 总结硬中断时间</h2>
<h2 id="bpftrace-进行cpu分析的跟踪程序">bpftrace 进行cpu分析的跟踪程序</h2>
<h2 id="offcputime">offcputime</h2>
<h2 id="syscount">syscount</h2>
<h2 id="runqslower">runqslower</h2>
<h2 id="cpufreq">cpufreq</h2>
<h2 id="smpcalls">smpcalls</h2>
<h2 id="llcstat">llcstat</h2>
<h2 id="oprofile">oprofile</h2>
<h2 id="atop">atop</h2>
<h2 id="proccpuinfo">/proc/cpuinfo</h2>
<h2 id="lscpu-1">lscpu</h2>
<h2 id="lstopo">lstopo</h2>
<h2 id="cpupower">cpupower</h2>
<h2 id="getdelays">getdelays</h2>
<pre tabindex="0"><code>getdelays工具是一个用户态工具，这个工具可以显示出指定pid或者tgid对应的调度延时数据
包括用户态内核态运行的时间，在就绪队列上等待运行的时间，以及等待IO等资源的延迟时间。
这些数据是通过netlink机制从内核获取，最终呈现给用户态。
</code></pre><h1 id="valgrind">valgrind</h1>
<h2 id="gpu">GPU</h2>
<pre tabindex="0"><code>nvidia-smi、nvperf、Nvidia VisualProfiler
# 适用于Nvidia GPU

intel_gpu_top、Intel vTune
# 适用于Intel GPU

radeontop
# 适用于Radoen GPU
</code></pre><h1 id="包">包</h1>
<h2 id="sysstat-系统资源监控">sysstat 系统资源监控</h2>
<pre tabindex="0"><code>sysstat是一个非常方便的工具，它带有众多的系统资源监控工具，用于监控系统的性能和使用情况。
我们在日常使用的工具中有相当一部分是来自sysstat工具包的。同时，它还提供了一种使用cron表达式来制定性能和活动数据的收集计划

apt-get install  sysstat
</code></pre><h2 id="stress-压力测试">stress 压力测试</h2>
<pre tabindex="0"><code>stress是Linux的一个压力测试工具，可以对CPU、Memory、IO、磁盘进行压力测试
sysstat 同时包含了常用的 Linux 性能工具，用来监控和分析系统的性能
案例会用到这个包的两个命令 mpstat 和 pidstat

yum install -y stress

我看到有些人用这个工具来描述一些资源耗尽的场景，也有人用它来做混沌测试中
请使用者要注意，这个工具并不是模拟业务问题的，是模拟系统级问题的
所以用它来模拟的时候，和业务真实场景差别还是很大的
通过这些源码说明，请你在使用的时候，要注意一下，像这样的工具，如果说只是为了单纯地消耗系统级的资源
然后观察应用在较少的可用资源下的表现如何，这样的工具是可以用的
但是如果是想要模拟你的业务层出现的问题，那我劝你还是别用这样的工具了

模拟cpu
模拟一个cpu  运行600秒
$ stress --cpu 1 --timeout 600
</code></pre><h2 id="stress-ng">stress-ng</h2>
<pre tabindex="0"><code>stress-ng完全兼容stress, 并且在stress基础上增加数百个选项参数，支持产生各种复杂的压力
</code></pre><h2 id="sysbench">sysbench</h2>
<pre tabindex="0"><code>sysbench 是一个多线程的基准测试工具，一般用来评估不同系统参数下的数据库负载情况

apt install sysbench sysstat


# 以10个线程运行5分钟的基准测试，模拟多线程切换的问题
$ sysbench --threads=10 --max-time=300 threads run

stress和sysbench两个工具在压测过程中的对比发现：
stress基于多进程的，会fork多个进程，导致进程上下文切换，导致us开销很高；
sysbench基于多线程的，会创建多个线程，单一进程基于内核线程切换，导致sy的内核开销很高；
具体可以通过vmstat对比
stress -c 8 -i 16 -t 600
vmstat 1 5
sysbench --threads=20 --time=300 threads run
vmstat 1 5
</code></pre><h2 id="ab">ab</h2>
<pre tabindex="0"><code>ab（apache bench）是一个常用的 HTTP 服务性能测试工具，这里用来模拟 Ngnix 的客户端。


# 并发10个请求测试Nginx性能，总共测试100个请求
$ ab -c 10 -n 100 http://192.168.0.10:10000/
This is ApacheBench, Version 2.3 &lt;$Revision: 1706008 $&gt;
Copyright 1996 Adam Twiss, Zeus Technology Ltd, 
...
Requests per second:    11.63 [#/sec] (mean)
Time per request:       859.942 [ms] (mean)
</code></pre><h2 id="gdb">GDB</h2>
<pre tabindex="0"><code></code></pre><h1 id="监控">监控</h1>
<h2 id="execsnoop-短进程分析">execsnoop 短进程分析</h2>
<pre tabindex="0"><code>在实际工作中，偶尔会遇到系统的CPU使用率和系统平均负载很高，但却找不到高CPU的应用
产生这个问题的原因：进程有可能在不断的崩溃、重启
通过uptime发现系统负载很高，但是通过top，mpstat，pidstat，perf等工具很难发现是什么进程导致了系统负载和CPU使用率很高
注：通过上面工具的判断，即不是CPU密集型，也不存在IO等待，也不存在进程、线程争用的情况
execsnoop-专门用于为追踪短时进程（瞬时进程）设计的工具
它通过 ftrace 实时监控进程的 exec() 行为，并输出短时进程的基本信息，包括进程 PID、父进程 PID、命令行参数以及执行的结果
github地址： https://github.com/brendangregg/perf-tools/blob/master/execsnoop
如何安装使用：将上面的github的内容复制，然后写入execsnoop文件，并且加上x权限即可

execsnoop 所用的 ftrace 是一种常用的动态追踪技术，一般用于分析 Linux 内核的运行时行为
</code></pre><h1 id="memory">Memory</h1>
<h1 id="概念-1">概念</h1>
<h2 id="术语">术语</h2>
<h2 id="buffer缓冲cache缓存">buffer缓冲cache缓存</h2>
<pre tabindex="0"><code>从 free 的手册中，你可以看到 buffer 和 cache 的说明。
Buffers 是内核缓冲区用到的内存，对应的是 /proc/meminfo 中的 Buffers 值。
Cache 是内核页缓存和 Slab 用到的内存，
对应的是 /proc/meminfo 中的 Cached 与 SReclaimable 之和。

Buffers 是对原始磁盘块的临时存储，也就是用来缓存磁盘的数据，通常不会特别大（20MB 左右）。
这样，内核就可以把分散的写集中起来，统一优化磁盘的写入，比如可以把多次小的写合并成单次大的写等等。

Cached 是从磁盘读取文件的页缓存，也就是用来缓存从文件读取的数据。
这样，下次访问这些文件数据时，就可以直接从内存中快速获取，而不需要再次访问缓慢的磁盘。

SReclaimable 是 Slab 的一部分。
Slab 包括两部分，
其中的可回收部分，用 SReclaimable 记录；
而不可回收部分，用 SUnreclaim 记录。

写文件时会用到 Cache 缓存数据，而写磁盘则会用到 Buffer 来缓存数据。
虽然文档上只提到，Cache 是文件读的缓存，但实际上，Cache 也会缓存写文件时的数据。

读文件时数据会缓存到 Cache 中，而读磁盘时数据会缓存到 Buffer 中。

Buffer 既可以用作“将要写入磁盘数据的缓存”，也可以用作“从磁盘读取数据的缓存”。
Cache 既可以用作“从文件读取数据的页缓存”，也可以用作“写文件的页缓存”。

Buffer 是对磁盘数据的缓存，而 Cache 是文件数据的缓存，它们既会用在读请求中，也会用在写请求中。
</code></pre><h2 id="缓存命中率">缓存命中率</h2>
<pre tabindex="0"><code>缓存的命中率。所谓缓存命中率，是指直接通过缓存获取数据的请求次数，占所有数据请求次数的百分比。
命中率越高，表示使用缓存带来的收益越高，应用程序的性能也就越好。
</code></pre><h2 id="free">free</h2>
<pre tabindex="0"><code>
# 注意不同版本的free输出可能会有所不同
$ free
              total        used        free      shared  buff/cache   available
Mem:        8169348      263524     6875352         668     1030472     7611064
Swap:             0           0           0

第一列，total 是总内存大小；
第二列，used 是已使用内存的大小，包含了共享内存；
第三列，free 是未使用内存的大小；
第四列，shared 是共享内存的大小；
第五列，buff/cache 是缓存和缓冲区的大小；
最后一列，available 是新进程可用内存的大小。
这里尤其注意一下，最后一列的可用内存 available 。
available 不仅包含未使用内存，还包括了可回收的缓存，所以一般会比未使用内存更大。
不过，并不是所有缓存都可以回收，因为有些缓存可能正在使用中
</code></pre><h2 id="cachestat">cachestat</h2>
<pre tabindex="0"><code>提供了整个操作系统缓存的读写命中情况
</code></pre><h2 id="cachetop">cachetop</h2>
<pre tabindex="0"><code>提供了每个进程的缓存命中情况
</code></pre><h2 id="vmstat">vmstat</h2>
<h2 id="psi">PSI</h2>
<h2 id="swapon">swapon</h2>
<h2 id="slabtop">slabtop</h2>
<h2 id="numastat">numastat</h2>
<h2 id="pmap">pmap</h2>
<h2 id="drsnoop">drsnoop</h2>
<h2 id="wss">wss</h2>
<h2 id="bpftrace">bpftrace</h2>
</div>

    
    
    

    
    
        <h4 class="page-header">Comments</h4>
        <div id="disqus_thread"></div>
<script type="application/javascript">
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "username" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    

</main>

        <footer>
            <p class="copyright text-muted">© All rights reserved. Powered by <a href="https://gohugo.io">Hugo</a> and <a href="https://github.com/calintat/minimal">Minimal</a>.</p>
        </footer>

        

        
    </body>

</html>

